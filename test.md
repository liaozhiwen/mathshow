# Autograd: è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶

PyTorch ä¸­æ‰€æœ‰ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ˜¯ ğŸ›ï¸autograd åŒ…ã€‚ æˆ‘ä»¬å…ˆç®€å•ä»‹ç»ä¸€ä¸‹è¿™ä¸ªåŒ…ï¼Œç„¶åè®­
ç»ƒç¬¬ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œã€‚

autograd åŒ…ä¸ºå¼ é‡ä¸Šçš„æ‰€æœ‰æ“ä½œæä¾›äº†è‡ªåŠ¨æ±‚å¯¼ã€‚ å®ƒæ˜¯ä¸€ä¸ªåœ¨è¿è¡Œæ—¶å®šä¹‰çš„æ¡†æ¶ï¼Œè¿™æ„å‘³ç€åå‘ä¼ æ’­æ˜¯æ ¹æ®ä½ çš„ä»£ç æ¥ç¡®å®šå¦‚ä½•è¿è¡Œï¼Œå¹¶ä¸”æ¯æ¬¡è¿­ä»£å¯ä»¥æ˜¯ä¸åŒçš„ã€‚

ğŸ—ï¸ ç¤ºä¾‹

ğŸ“’ å¼ é‡ï¼ˆTensorï¼‰
ğŸ’­ torch.Tensor æ˜¯è¿™ä¸ªåŒ…çš„æ ¸å¿ƒç±»ã€‚å¦‚æœè®¾ç½® .requires_gradğŸ”§ ä¸º Trueï¼Œé‚£ä¹ˆå°†ä¼šè¿½è¸ªæ‰€æœ‰å¯¹äºè¯¥å¼ é‡çš„æ“ä½œã€‚ å½“å®Œæˆè®¡ç®—åé€šè¿‡è°ƒç”¨ .backward()ï¼Œè‡ªåŠ¨è®¡ç®—æ‰€æœ‰çš„æ¢¯åº¦ï¼Œè¿™ä¸ªå¼ é‡çš„æ‰€æœ‰æ¢¯åº¦å°†ä¼šè‡ªåŠ¨ç§¯ç´¯åˆ° .grad å±æ€§ ğŸ”§ã€‚

- é˜»æ­¢å¼ é‡è·Ÿè¸ªå†å²è®°å½•
  è¦é˜»æ­¢å¼ é‡è·Ÿè¸ªå†å²è®°å½•ï¼Œå¯ä»¥è°ƒç”¨.detach()æ–¹æ³• âœ‹ å°†å…¶ä¸è®¡ç®—å†å²è®°å½•åˆ†ç¦»ï¼Œå¹¶ç¦æ­¢è·Ÿè¸ªå®ƒå°†æ¥çš„è®¡ç®—è®°å½•ã€‚

ä¸ºäº†é˜²æ­¢è·Ÿè¸ªå†å²è®°å½•ï¼ˆå’Œä½¿ç”¨å†…å­˜ï¼‰ï¼Œå¯ä»¥å°†ä»£ç å—åŒ…è£…åœ¨ with torch.no_grad()ï¼šä¸­ã€‚ åœ¨è¯„ä¼°æ¨¡å‹æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºæ¨¡å‹å¯èƒ½å…·æœ‰ requires_grad = True çš„å¯è®­ç»ƒå‚æ•°ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸éœ€è¦æ¢¯åº¦è®¡ç®—ã€‚

åœ¨è‡ªåŠ¨æ¢¯åº¦è®¡ç®—ä¸­è¿˜æœ‰å¦å¤–ä¸€ä¸ªé‡è¦çš„ç±» ğŸ’­Function.

Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a .grad_fnğŸ”§ attribute that references a Function that has created the ğŸ“’Tensor (except for Tensors created by the user - their grad_fn is None).

Tensor å’Œ Function äº’ç›¸è¿æ¥å¹¶ç”Ÿæˆä¸€ä¸ªéå¾ªç¯å›¾ï¼Œå®ƒè¡¨ç¤ºå’Œå­˜å‚¨äº†å®Œæ•´çš„è®¡ç®—å†å²ã€‚ æ¯ä¸ªå¼ é‡éƒ½æœ‰ä¸€ä¸ª.grad_fn å±æ€§ ğŸ”§ï¼Œè¿™ä¸ªå±æ€§å¼•ç”¨äº†ä¸€ä¸ªåˆ›å»ºäº† Tensor çš„ Functionï¼ˆé™¤éè¿™ä¸ªå¼ é‡æ˜¯ç”¨æˆ·æ‰‹åŠ¨åˆ›å»ºçš„ï¼Œå³ï¼Œè¿™ä¸ªå¼ é‡çš„ grad_fn æ˜¯ Noneï¼‰ã€‚

å¦‚æœéœ€è¦è®¡ç®—å¯¼æ•°$d$ï¼Œä½ å¯ä»¥åœ¨ Tensor ä¸Šè°ƒç”¨.backward()âœ‹ã€‚ å¦‚æœ Tensor æ˜¯ä¸€ä¸ªæ ‡é‡ï¼ˆå³å®ƒåŒ…å«ä¸€ä¸ªå…ƒç´ æ•°æ®ï¼‰åˆ™ä¸éœ€è¦ä¸º backward()æŒ‡å®šä»»ä½•å‚æ•°ï¼Œ ä½†æ˜¯å¦‚æœå®ƒæœ‰æ›´å¤šçš„å…ƒç´ ï¼Œä½ éœ€è¦æŒ‡å®šä¸€ä¸ª gradient å‚æ•° ğŸˆ æ¥åŒ¹é…å¼ é‡çš„å½¢çŠ¶ã€‚

è¯‘è€…æ³¨ï¼šåœ¨å…¶ä»–çš„æ–‡ç« ä¸­ä½ å¯èƒ½ä¼šçœ‹åˆ°è¯´å°† Tensor åŒ…è£¹åˆ° Variable ä¸­æä¾›è‡ªåŠ¨æ¢¯åº¦è®¡ç®—ï¼ŒVariable è¿™ä¸ªåœ¨ 0.41 ç‰ˆä¸­å·²ç»è¢«æ ‡æ³¨ä¸ºè¿‡æœŸäº†ï¼Œç°åœ¨å¯ä»¥ç›´æ¥ä½¿ç”¨ Tensorï¼Œ[å®˜æ–¹æ–‡æ¡£åœ¨è¿™é‡Œ](https://pytorch.org/docs/stable/autograd.html#variable-deprecated)

å…·ä½“çš„åé¢ä¼šæœ‰è¯¦ç»†è¯´æ˜

```py
import torch

# åˆ›å»ºä¸€ä¸ªå¼ é‡å¹¶è®¾ç½® requires_grad=True ç”¨æ¥è¿½è¸ªä»–çš„è®¡ç®—å†å²
x = torch.ones(2, 2, requires_grad=True)
print(x)

[out]
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)

# å¯¹å¼ é‡è¿›è¡Œæ“ä½œ:
y = x + 2
print(y)

[out]
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward>)

# ç»“æœ y å·²ç»è¢«è®¡ç®—å‡ºæ¥äº†ï¼Œæ‰€ä»¥ï¼Œgrad_fn å·²ç»è¢«è‡ªåŠ¨ç”Ÿæˆäº†ã€‚
print(y.grad_fn)

[out]
<AddBackward object at 0x00000232535FD860>

# å¯¹ y è¿›è¡Œä¸€ä¸ªæ“ä½œ
z = y _ y _ 3
out = z.mean()

print(z, out)

[out]
tensor([[27., 27.],
        [27., 27.]], grad*fn=<MulBackward>)
        tensor(27., grad_fn=<MeanBackward1>)

# .requires_grad\*( ... ) å¯ä»¥æ”¹å˜ç°æœ‰å¼ é‡çš„ requires_grad å±æ€§ã€‚ å¦‚æœæ²¡æœ‰æŒ‡å®šçš„è¯ï¼Œé»˜è®¤è¾“å…¥çš„ flag æ˜¯ Falseã€‚
a = torch.randn(2, 2)
a = ((a _ 3) / (a - 1))
print(a.requires*grad)
a.requires_grad*(True)
print(a.requires_grad)
b = (a _ a).sum()
print(b.grad_fn)

[out]
False
True
<SumBackward0 object at 0x000002325360B438>
```

## æ¢¯åº¦

```py
åå‘ä¼ æ’­ å› ä¸º out æ˜¯ä¸€ä¸ªçº¯é‡ï¼ˆscalarï¼‰ï¼Œout.backward() ç­‰äº out.backward(torch.tensor(1))ã€‚

out.backward()
[out]
print gradients d(out)/dx

print(x.grad)

[out]
    tensor([[4.5000, 4.5000],
            [4.5000, 4.5000]])

å¾—åˆ°çŸ©é˜µ 4.5.è°ƒç”¨ out Tensor.
```

$$
o
$$

å¾—åˆ°

$$o = \frac{1}{4}\sum_i z_i$$

$$z_i = 3(x_i+2)^2$$

and

$$z_i \big|_{x_i=1} = 27$$

å› æ­¤,

$$
$$

, hence
$$\frac{\partial o}{\partial x*i}\bigr\rvert*{x_i=1} = \frac{9}{2} = 4.5$$

å¯ä»¥ä½¿ç”¨ autograd åšæ›´å¤šçš„æ“ä½œ

```py
x = torch.randn(3, requires_grad=True)

y = x _ 2
while y.data.norm() < 1000:
y = y _ 2

print(y)

[out]
tensor([-920.6895, -115.7301, -867.6995], grad_fn=<MulBackward>)


gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(gradients)

print(x.grad)
tensor([ 51.2000, 512.0000, 0.0512])
```

å¦‚æœ.requires_grad=True ä½†æ˜¯ä½ åˆä¸å¸Œæœ›è¿›è¡Œ autograd çš„è®¡ç®—ï¼Œ é‚£ä¹ˆå¯ä»¥å°†å˜é‡åŒ…è£¹åœ¨ with torch.no_grad()ä¸­:

```py
print(x.requires_grad)
print((x \*\* 2).requires_grad)

with torch.no_grad():
print((x \*\* 2).requires_grad)

[out]
True
True
False
```

ç¨åé˜…è¯»:

autograd å’Œ Function çš„å®˜æ–¹æ–‡æ¡£ https://pytorch.org/docs/autograd
